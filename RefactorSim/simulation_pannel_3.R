# the code that actual call the function `perfect_match_trials` to generating the samples
# and running algrithm to identify DMRs

# load the necessary dependent files--------------------
load("SiteInfo.RData")
source("Function_dmrIdentifier_opt.R")
source("Function_perfect_match_trials.R")
sites.selected <- raw.sites[1:5e3, ]
get_cluster_lengths <- function(sites, maxGap = 1000, min.size = 3, max.size = 15) {
  # the function takes a sites file and some necessary parameters for generating
  # clusters by clusterMaker. It returns a numeric vector that record the size of each cluster. More
  # specifically, the ith position of the vector records how many CpG sites does Cluster i has.

  # sites : an file with MAPINFO of sites on a chromosome
  # maxGap: the parameter pass to clusterMaker to generate clusters
  # min.size: clusters with sites less than this will be discard
  # max.size: clusters with more sites than this will be discard
  require("bumphunter")

  cl <- bumphunter::clusterMaker(
    chr = sites$Chromosome_36,
    pos = sites$MAPINFO,
    maxGap = maxGap
  )
  cl <- sort(cl)
  cl.lst <- rle(cl)
  lengths <- cl.lst$lengths[cl.lst$lengths >= min.size & cl.lst$lengths <= max.size ]
  lengths <- sort(lengths)
  return(lengths)
}
cluster.lengths <- get_cluster_lengths(sites.selected)

# actual code used to call functions and generating samples--------------------------

# conducting experiments that only mean difference is present

lst.tab3.mean.a <- perfect_match_trials(mu0 = 0, mu1 = 1.0, delta = 1.0, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.mean.b <- perfect_match_trials(mu0 = 1, mu1 = 0.0, delta = 1.0, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.mean.c <- perfect_match_trials(mu0 = 0, mu1 = 1.5, delta = 1.0, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.mean.d <- perfect_match_trials(mu0 = 0, mu1 = 2.0, delta = 1.0, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
save(lst.tab3.mean.a, lst.tab3.mean.b, lst.tab3.mean.c, lst.tab3.mean.d, file = "Trial3_mean_only.RData")

# conducting experiments that only variance difference is present

lst.tab3.var.a <- perfect_match_trials(mu0 = 0, mu1 = 0.0, delta = 0.75, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.var.b <- perfect_match_trials(mu0 = 0, mu1 = 0.0, delta = 1.50, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.var.c <- perfect_match_trials(mu0 = 0, mu1 = 0.0, delta = 2.00, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.var.d <- perfect_match_trials(mu0 = 0, mu1 = 0.0, delta = 2.50, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
save(lst.tab3.var.a, lst.tab3.var.b, lst.tab3.var.c, lst.tab3.var.d, file = "Trial3_variance_only.RData")

# conducting experiments that both mean and variance are different

lst.tab3.mix.a <- perfect_match_trials(mu0 = 0, mu1 = 1.0, delta = 0.75, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.mix.b <- perfect_match_trials(mu0 = 0, mu1 = 1.5, delta = 0.75, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.mix.c <- perfect_match_trials(mu0 = 0, mu1 = 2.0, delta = 0.75, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.mix.d <- perfect_match_trials(mu0 = 0, mu1 = 1.0, delta = 1.25, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.mix.e <- perfect_match_trials(mu0 = 0, mu1 = 1.0, delta = 1.50, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)
lst.tab3.mix.f <- perfect_match_trials(mu0 = 0, mu1 = 1.0, delta = 2.00, sample.N = 20, num.pairs = 100, cluster.lengths = cluster.lengths, B = 250)


save(lst.tab3.mix.a, lst.tab3.mix.b, lst.tab3.mix.c, lst.tab3.mix.d,lst.tab3.mix.e, lst.tab3.mix.f, file = "Trial3_mix.RData")

load("./JamesStewardProject/RefactorSim/Trial3_mean_only.RData")

# define a function that  analyze the data.frames generated
perfect_match_test_evaluator <- function(lst.tabs, alpha = 0.05) {
# the function takes a list contains data.frames generated by perfect_match_trials() and returns a data.frame
# that gives an estimate of type1 error rate, true positive rate(TPR) and false postie rate (FPR)
# argument:
# lst.tabs: a list contains real.index and two lists of data.frames generated by Wang's algrithm on simulated perfect match data, called null.tabs and alt.tabs.
# There is no dmr simulated in data used to generate null.tabs; clusters specified by real.index are generated as DMR for the data used for generate
# alt.tabs
#
#alpha: a positive number used to specify the signifcant level. Candidate DMRs specified in the data.frame with fwer less than alpha will be
# considered as a dmr identifeid by the algrithm.

  null.tabs <- lst.tabs[["null.tabs"]]
  alt.tabs <- lst.tabs[["alt.tabs"]]
  real.index <- lst.tabs[["real.index"]]

  # estimate the type-1 error rate by calculate average number of clusters being incorrectly identified as DMRs over total clusters generated
  alpha <- 0.05 # the threhold
  sample.N <- length(null.tabs) # how many trials run in total
  num.clusters <- length(cluster.lengths) # total number of clusters
  df.combined.null.tabs <- do.call(rbind, null.tabs)
  identified.0 <- df.combined.null.tabs[df.combined.null.tabs$fwerArea < alpha, ]
  type1.error <- nrow(identified.0) / (num.clusters * sample.N)

  # calculate average FPR and TPR. TPR is calculated as the proportion of DMRs correctely identified among all DMRs identified and
  # FPR is the proportion of DMRs incorrectly identified among all clusters that are not simulated as DMR.
  lst.power.estimate.of.trial <- lapply(alt.tabs, function(df) {
    df.identified.positive <- df[df$fwerArea < alpha, ]
    if (nrow(df.identified.positive) == 0) {
      tpr <- 0
      fpr <- 0
    } else {
      df.true.positive <- df.identified.positive[df.identified.positive$cluster %in% real.index, ]
      tpr <- nrow(df.true.positive) / nrow(df.identified.positive)
      fpr <- (nrow(df.identified.positive) - nrow(df.true.positive)) / (num.clusters - length(real.index))
    }
    return(c(tpr,fpr))
  })

  df.combined.power.estimate <- do.call(rbind, lst.power.estimate.of.trial)
  avg.estimate <- c(type1.error, colMeans(df.combined.power.estimate))
  names(avg.estimate) <- c("type1.error", "average.TPR", "average.FPR")
  return(avg.estimate)
}

# run the function perfect_match_test_evaluator() on the simulating result generated
lst.evaluation.mean.only <- lapply( list(lst.tab3.mean.a, lst.tab3.mean.b, lst.tab3.mean.c, lst.tab3.mean.d), function(lst.tab) {
  eval.result <- perfect_match_test_evaluator(lst.tab,alpha = 0.05)
  return(eval.result)
})

tab.evaluation.mean.only <- do.call( rbind,lst.evaluation.mean.only )

lst.evaluation.variance.only <- lapply( list(lst.tab3.var.a, lst.tab3.var.b, lst.tab3.var.c, lst.tab3.var.d), function(lst.tab) {
  eval.result <- perfect_match_test_evaluator(lst.tab,alpha = 0.05)
  return(eval.result)
})

tab.evaluation.variance.only <- do.call( rbind,lst.evaluation.variance.only )

tab.evaluation.mean.only <- do.call( rbind,lst.evaluation.mean.only )

lst.evaluation.mix <- lapply( list(lst.tab3.mix.a, lst.tab3.mix.b, lst.tab3.mix.c, lst.tab3.mix.d,lst.tab3.mix.e,lst.tab3.mix.f), function(lst.tab) {
  eval.result <- perfect_match_test_evaluator(lst.tab,alpha = 0.05)
  return(eval.result)
})

tab.evaluation.mix <- do.call( rbind,lst.evaluation.mix)
